{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anohl\\OneDrive\\Dokumente\\A_Uni_stuff\\Albstadt\\Semester 2\\Computer_vision\\Aufgaben\\cv_class_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForObjectDetection, TrainingArguments, Trainer, YolosImageProcessor\n",
    "from PIL import Image, ImageDraw\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import time\n",
    "import configparser\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = configparser.ConfigParser()\n",
    "# config.read(\"config.ini\")\n",
    "# AUTH_TOKEN = config[\"auth\"][\"token\"]\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "IMAGE_PROCESSOR_GLOBAL = YolosImageProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validation/Test-Split abgeschlossen! Daten in 'c:\\Users\\anohl\\OneDrive\\Dokumente\\A_Uni_stuff\\Albstadt\\Semester 2\\Computer_vision\\Aufgaben\\Data\\Kugellager_Data\\YOLO_data' gespeichert.\n",
      "Train: 480 | Test: 120\n"
     ]
    }
   ],
   "source": [
    "def clear_directory(directory):\n",
    "    \"\"\"\n",
    "    Löscht den gesamten Inhalt eines Verzeichnisses, ohne das Verzeichnis selbst zu entfernen.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Pfad zum Verzeichnis.\n",
    "    \"\"\"\n",
    "    if os.path.exists(directory):\n",
    "        for item in os.listdir(directory):\n",
    "            item_path = os.path.join(directory, item)\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.unlink(item_path)  \n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)  \n",
    "\n",
    "\n",
    "def split_dataset(images_dir, labels_dir, output_dir, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Teilt einen Datensatz (Bilder + Labels) in Training, Validierung und Test auf.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): Pfad zum Ordner mit den Bildern.\n",
    "        labels_dir (str): Pfad zum Ordner mit den Labels.\n",
    "        output_dir (str): Pfad zum Ordner, in dem die aufgeteilten Daten gespeichert werden sollen.\n",
    "        val_ratio (float): Verhältnis der Validierungsdaten (zwischen 0 und 1). Standard: 0.1.\n",
    "        test_ratio (float): Verhältnis der Testdaten (zwischen 0 und 1). Standard: 0.1.\n",
    "        seed (int): Zufallssaat für Reproduzierbarkeit. Standard: 42.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Unterordner-Pfade\n",
    "    train_images_dir = os.path.join(output_dir, \"train/images\")\n",
    "    train_labels_dir = os.path.join(output_dir, \"train/labels\")\n",
    "    test_images_dir = os.path.join(output_dir, \"test/images\")\n",
    "    test_labels_dir = os.path.join(output_dir, \"test/labels\")\n",
    "    \n",
    "    # Ordner neu erstellen\n",
    "    os.makedirs(train_images_dir, exist_ok=True)\n",
    "    os.makedirs(train_labels_dir, exist_ok=True)\n",
    "    os.makedirs(test_images_dir, exist_ok=True)\n",
    "    os.makedirs(test_labels_dir, exist_ok=True)\n",
    "\n",
    "    for subdir in [train_images_dir, train_labels_dir, test_images_dir, test_labels_dir]:\n",
    "        clear_directory(subdir)\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "    # Liste aller Bilder\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    # Shuffle und Split\n",
    "    random.seed(seed)\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    test_split = int(len(image_files) * test_ratio)\n",
    "    \n",
    "    test_files = image_files[:test_split]\n",
    "    train_files = image_files[test_split:]\n",
    "\n",
    "    # Dateien kopieren\n",
    "    def copy_files(file_list, dest_images_dir, dest_labels_dir):\n",
    "        for file in file_list:\n",
    "            shutil.copy(os.path.join(images_dir, file), os.path.join(dest_images_dir, file))\n",
    "            label_file = file.rsplit('.', 1)[0] + '.txt'\n",
    "            if os.path.exists(os.path.join(labels_dir, label_file)):\n",
    "                shutil.copy(os.path.join(labels_dir, label_file), os.path.join(dest_labels_dir, label_file))\n",
    "\n",
    "    copy_files(train_files, train_images_dir, train_labels_dir)\n",
    "    copy_files(test_files, test_images_dir, test_labels_dir)\n",
    "\n",
    "    print(f\"Train/Validation/Test-Split abgeschlossen! Daten in '{output_dir}' gespeichert.\")\n",
    "    print(f\"Train: {len(train_files)} | Test: {len(test_files)}\")\n",
    "\n",
    "yolo_images_dir = os.path.join(BASE_DIR, \"Data\", \"Kugellager_Data\", \"YOLO_data\", \"yolo_images_dump\")\n",
    "yolo_labels_dir = os.path.join(BASE_DIR, \"Data\", \"Kugellager_Data\", \"YOLO_data\", \"yolo_labels_dump\")\n",
    "yolo_output_dir = os.path.join(BASE_DIR, \"Data\", \"Kugellager_Data\", \"YOLO_data\")\n",
    "\n",
    "split_dataset(images_dir=yolo_images_dir,\n",
    "              labels_dir=yolo_labels_dir,\n",
    "              output_dir=yolo_output_dir,\n",
    "              test_ratio=0.2,\n",
    "              seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_to_custom_format(images_dir, labels_dir, categories):\n",
    "    \"\"\"\n",
    "    Converts YOLO annotations to a custom dataset format similar to CPPE-5.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): Path to the images directory.\n",
    "        labels_dir (str): Path to the YOLO labels directory.\n",
    "        categories (list): List of category names.\n",
    "\n",
    "    Returns:\n",
    "        list: A dataset where each entry contains image metadata and associated objects.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "\n",
    "    for image_file in sorted(os.listdir(images_dir)):\n",
    "        if not image_file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "\n",
    "        # Open the image as a PIL image object\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "\n",
    "            # Prepare the image entry\n",
    "            image_entry = {\n",
    "                'image_id': image_id,\n",
    "                'image': img.copy(),  # Keep a reference to the PIL image\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'objects': {\n",
    "                    'id': [],\n",
    "                    'area': [],\n",
    "                    'bbox': [],\n",
    "                    'category': []\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Corresponding label file in YOLO format\n",
    "            label_file = os.path.join(labels_dir, image_file.rsplit('.', 1)[0] + '.txt')\n",
    "            if os.path.exists(label_file):\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        category_id = int(parts[0])\n",
    "                        x_center, y_center, box_width, box_height = map(float, parts[1:])\n",
    "\n",
    "                        # Convert YOLO to bounding box coordinates\n",
    "                        x_min = (x_center - box_width / 2) * width\n",
    "                        y_min = (y_center - box_height / 2) * height\n",
    "                        bbox_width = box_width * width\n",
    "                        bbox_height = box_height * height\n",
    "                        area = bbox_width * bbox_height\n",
    "\n",
    "                        # Append object data\n",
    "                        image_entry['objects']['id'].append(annotation_id)\n",
    "                        image_entry['objects']['area'].append(int(area))\n",
    "                        image_entry['objects']['bbox'].append([\n",
    "                            round(x_min, 1),\n",
    "                            round(y_min, 1),\n",
    "                            round(bbox_width, 1),\n",
    "                            round(bbox_height, 1)\n",
    "                        ])\n",
    "                        image_entry['objects']['category'].append(category_id)\n",
    "\n",
    "                        annotation_id += 1\n",
    "\n",
    "            dataset.append(image_entry)\n",
    "            image_id += 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "categories_kugellager = [\n",
    "    {\"id\": 0, \"name\": \"defect\"},\n",
    "]\n",
    "images_dir_kugellager_train = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/train/images\")\n",
    "labels_dir_kugellager_train = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/train/labels\")\n",
    "\n",
    "dataset_kugellager_train = Dataset.from_list(convert_yolo_to_custom_format(images_dir_kugellager_train, labels_dir_kugellager_train, categories_kugellager))\n",
    "\n",
    "images_dir_kugellager_test = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/test/images\")\n",
    "labels_dir_kugellager_test = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/test/labels\")\n",
    "\n",
    "dataset_kugellager_test = Dataset.from_list(convert_yolo_to_custom_format(images_dir_kugellager_test, labels_dir_kugellager_test, categories_kugellager))\n",
    "\n",
    "\n",
    "\n",
    "images_dir_kugellager_train_halb = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/train_halb/images\")\n",
    "labels_dir_kugellager_train_halb = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/train_halb/labels\")\n",
    "\n",
    "dataset_kugellager_train_halb = Dataset.from_list(convert_yolo_to_custom_format(images_dir_kugellager_train_halb, labels_dir_kugellager_train_halb, categories_kugellager))\n",
    "\n",
    "images_dir_kugellager_test_halb = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/test_halb/images\")\n",
    "labels_dir_kugellager_test_halb = os.path.join(BASE_DIR, \"Data/Kugellager_Data/YOLO_Data/test_halb/labels\")\n",
    "\n",
    "dataset_kugellager_test_halb = Dataset.from_list(convert_yolo_to_custom_format(images_dir_kugellager_test_halb, labels_dir_kugellager_test_halb, categories_kugellager))\n",
    "\n",
    "categories_oberfläche = [\n",
    "    {\"id\": 0, \"name\": \"crazing\"},\n",
    "    {\"id\": 1, \"name\": \"inclusion\"},\n",
    "    {\"id\": 2, \"name\": \"patches\"},\n",
    "    {\"id\": 3, \"name\": \"pitted surface\"},\n",
    "    {\"id\": 4, \"name\": \"rolled in scale\"},\n",
    "    {\"id\": 5, \"name\": \"scratches\"}\n",
    "]\n",
    "\n",
    "images_dir_oberfläche_train = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/train/images\")\n",
    "labels_dir_oberfläche_train = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/train/labels\")\n",
    "\n",
    "dataset_oberfläche_train = Dataset.from_list(convert_yolo_to_custom_format(images_dir_oberfläche_train, labels_dir_oberfläche_train, categories_oberfläche))\n",
    "\n",
    "images_dir_oberfläche_test = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/test/images\")\n",
    "labels_dir_oberfläche_test = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/test/labels\")\n",
    "\n",
    "dataset_oberfläche_test = Dataset.from_list(convert_yolo_to_custom_format(images_dir_oberfläche_test, labels_dir_oberfläche_test, categories_oberfläche))\n",
    "\n",
    "\n",
    "\n",
    "images_dir_oberfläche_train_halb = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/train_halb/images\")\n",
    "labels_dir_oberfläche_train_halb = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/train_halb/labels\")\n",
    "\n",
    "dataset_oberfläche_train_halb = Dataset.from_list(convert_yolo_to_custom_format(images_dir_oberfläche_train_halb, labels_dir_oberfläche_train_halb, categories_oberfläche))\n",
    "\n",
    "images_dir_oberfläche_test_halb = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/test_halb/images\")\n",
    "labels_dir_oberfläche_test_halb = os.path.join(BASE_DIR, \"Data/Oberflächen_Data/YOLO_Data/test_halb/labels\")\n",
    "\n",
    "dataset_oberfläche_test_halb = Dataset.from_list(convert_yolo_to_custom_format(images_dir_oberfläche_test_halb, labels_dir_oberfläche_test_halb, categories_oberfläche))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device name: NVIDIA GeForce RTX 3060 Ti\n",
      "CUDA version: 12.4\n",
      "cuDNN version: 90100\n",
      "Gesamtspeicher: 8.59 GB\n",
      "Belegter Speicher: 0.00 GB\n",
      "Zwischengespeicherter Speicher: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Das hier war lediglich Code um ein memory Problem zu lösen, das beim Training auftrat\n",
    "\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "# print(\"CUDA version:\", torch.version.cuda)\n",
    "# print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "# total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "# # Belegter Speicher (in Bytes)\n",
    "# allocated_memory = torch.cuda.memory_allocated(0)\n",
    "\n",
    "# # Zwischengespeicherter Speicher\n",
    "# cached_memory = torch.cuda.memory_reserved(0)\n",
    "\n",
    "# print(f\"Gesamtspeicher: {total_memory / 1e9:.2f} GB\")\n",
    "# print(f\"Belegter Speicher: {allocated_memory / 1e9:.2f} GB\")\n",
    "# print(f\"Zwischengespeicherter Speicher: {cached_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anohl\\AppData\\Local\\Temp\\ipykernel_22816\\3540235056.py:89: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/1440 [00:00<?, ?it/s]The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  1%|          | 10/1440 [00:10<22:03,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.966, 'grad_norm': 86.73352813720703, 'learning_rate': 4.965277777777778e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 20/1440 [00:19<21:46,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2447, 'grad_norm': 105.39154052734375, 'learning_rate': 4.930555555555556e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 30/1440 [00:28<21:39,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6343, 'grad_norm': 52.249420166015625, 'learning_rate': 4.8958333333333335e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 40/1440 [00:38<21:45,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3955, 'grad_norm': 153.78778076171875, 'learning_rate': 4.8611111111111115e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  3%|▎         | 48/1440 [00:52<19:29,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3481593132019043, 'eval_runtime': 7.0429, 'eval_samples_per_second': 17.038, 'eval_steps_per_second': 2.13, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 50/1440 [00:54<56:09,  2.42s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4114, 'grad_norm': 68.31089782714844, 'learning_rate': 4.8263888888888895e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 60/1440 [01:03<22:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9912, 'grad_norm': 189.64048767089844, 'learning_rate': 4.791666666666667e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 70/1440 [01:12<20:52,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8774, 'grad_norm': 1266.1068115234375, 'learning_rate': 4.756944444444444e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 80/1440 [01:21<20:42,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.585, 'grad_norm': 106.85755157470703, 'learning_rate': 4.722222222222222e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 90/1440 [01:30<20:29,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3483, 'grad_norm': 67.62145233154297, 'learning_rate': 4.6875e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  7%|▋         | 96/1440 [01:43<18:40,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.344599962234497, 'eval_runtime': 7.1639, 'eval_samples_per_second': 16.751, 'eval_steps_per_second': 2.094, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 100/1440 [01:47<37:09,  1.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3344, 'grad_norm': 48.39333724975586, 'learning_rate': 4.652777777777778e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 110/1440 [01:56<20:50,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5104, 'grad_norm': 72.16051483154297, 'learning_rate': 4.618055555555556e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 120/1440 [02:05<20:17,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1418, 'grad_norm': 130.7357635498047, 'learning_rate': 4.5833333333333334e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 130/1440 [02:14<18:51,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5798, 'grad_norm': 102.42346954345703, 'learning_rate': 4.5486111111111114e-05, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 140/1440 [02:22<18:15,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3239, 'grad_norm': 54.39695739746094, 'learning_rate': 4.5138888888888894e-05, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 144/1440 [02:32<16:32,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.422067642211914, 'eval_runtime': 6.7463, 'eval_samples_per_second': 17.788, 'eval_steps_per_second': 2.223, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 150/1440 [02:38<25:57,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2925, 'grad_norm': 118.2507095336914, 'learning_rate': 4.4791666666666673e-05, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 160/1440 [02:46<18:29,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3207, 'grad_norm': 48.24990463256836, 'learning_rate': 4.4444444444444447e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 170/1440 [02:55<18:02,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3703, 'grad_norm': 39.34617233276367, 'learning_rate': 4.4097222222222226e-05, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 180/1440 [03:03<17:51,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5313, 'grad_norm': 27.763797760009766, 'learning_rate': 4.375e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 190/1440 [03:12<17:36,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3814, 'grad_norm': 58.21919631958008, 'learning_rate': 4.340277777777778e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 13%|█▎        | 192/1440 [03:20<15:53,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2514584064483643, 'eval_runtime': 6.8009, 'eval_samples_per_second': 17.645, 'eval_steps_per_second': 2.206, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 200/1440 [03:27<21:14,  1.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4668, 'grad_norm': 26.908164978027344, 'learning_rate': 4.305555555555556e-05, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 210/1440 [03:36<17:35,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4813, 'grad_norm': 113.93665313720703, 'learning_rate': 4.270833333333333e-05, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 220/1440 [03:44<17:22,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5085, 'grad_norm': 244.6085662841797, 'learning_rate': 4.236111111111111e-05, 'epoch': 4.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 230/1440 [03:53<17:11,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2131, 'grad_norm': 65.60359954833984, 'learning_rate': 4.201388888888889e-05, 'epoch': 4.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 240/1440 [04:01<15:21,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5644, 'grad_norm': 112.4634780883789, 'learning_rate': 4.166666666666667e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 17%|█▋        | 240/1440 [04:08<15:21,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5201363563537598, 'eval_runtime': 6.8356, 'eval_samples_per_second': 17.555, 'eval_steps_per_second': 2.194, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 250/1440 [04:17<18:38,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4313, 'grad_norm': 52.912208557128906, 'learning_rate': 4.1319444444444445e-05, 'epoch': 5.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 260/1440 [04:25<16:50,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3916, 'grad_norm': 212.3209686279297, 'learning_rate': 4.0972222222222225e-05, 'epoch': 5.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 270/1440 [04:34<16:34,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3894, 'grad_norm': 46.04008102416992, 'learning_rate': 4.0625000000000005e-05, 'epoch': 5.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 280/1440 [04:42<16:29,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3398, 'grad_norm': 57.420318603515625, 'learning_rate': 4.027777777777778e-05, 'epoch': 5.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 288/1440 [04:56<14:52,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.410149335861206, 'eval_runtime': 6.8863, 'eval_samples_per_second': 17.426, 'eval_steps_per_second': 2.178, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 290/1440 [04:58<44:52,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.341, 'grad_norm': 54.31477737426758, 'learning_rate': 3.993055555555556e-05, 'epoch': 6.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 300/1440 [05:07<16:59,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.43, 'grad_norm': 80.29505157470703, 'learning_rate': 3.958333333333333e-05, 'epoch': 6.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 310/1440 [05:15<15:58,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1825, 'grad_norm': 58.659942626953125, 'learning_rate': 3.923611111111111e-05, 'epoch': 6.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 320/1440 [05:24<15:57,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3833, 'grad_norm': 54.139801025390625, 'learning_rate': 3.888888888888889e-05, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 330/1440 [05:32<15:53,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2381, 'grad_norm': 41.790672302246094, 'learning_rate': 3.854166666666667e-05, 'epoch': 6.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 23%|██▎       | 336/1440 [05:44<14:10,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3157793283462524, 'eval_runtime': 6.877, 'eval_samples_per_second': 17.449, 'eval_steps_per_second': 2.181, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 340/1440 [05:48<28:53,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1438, 'grad_norm': 39.83462142944336, 'learning_rate': 3.8194444444444444e-05, 'epoch': 7.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 350/1440 [05:56<15:49,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3041, 'grad_norm': 74.752197265625, 'learning_rate': 3.7847222222222224e-05, 'epoch': 7.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 360/1440 [06:05<15:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2434, 'grad_norm': 55.10074234008789, 'learning_rate': 3.7500000000000003e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 370/1440 [06:13<15:10,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1877, 'grad_norm': 50.95230484008789, 'learning_rate': 3.715277777777778e-05, 'epoch': 7.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 380/1440 [06:22<15:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0416, 'grad_norm': 51.06264877319336, 'learning_rate': 3.6805555555555556e-05, 'epoch': 7.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 27%|██▋       | 384/1440 [06:32<13:29,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3323194980621338, 'eval_runtime': 6.8299, 'eval_samples_per_second': 17.57, 'eval_steps_per_second': 2.196, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 390/1440 [06:37<20:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2901, 'grad_norm': 79.8580322265625, 'learning_rate': 3.6458333333333336e-05, 'epoch': 8.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 400/1440 [06:46<14:52,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9776, 'grad_norm': 51.2769889831543, 'learning_rate': 3.611111111111111e-05, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 410/1440 [06:54<14:36,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4913, 'grad_norm': 161.80548095703125, 'learning_rate': 3.576388888888889e-05, 'epoch': 8.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 420/1440 [07:03<14:31,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4092, 'grad_norm': 32.18500900268555, 'learning_rate': 3.541666666666667e-05, 'epoch': 8.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 430/1440 [07:12<14:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3412, 'grad_norm': 53.025672912597656, 'learning_rate': 3.506944444444444e-05, 'epoch': 8.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 30%|███       | 432/1440 [07:20<12:58,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.360055923461914, 'eval_runtime': 6.7677, 'eval_samples_per_second': 17.731, 'eval_steps_per_second': 2.216, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 440/1440 [07:27<17:01,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3856, 'grad_norm': 53.259544372558594, 'learning_rate': 3.472222222222222e-05, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 450/1440 [07:35<14:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2959, 'grad_norm': 39.12895202636719, 'learning_rate': 3.4375e-05, 'epoch': 9.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 460/1440 [07:44<14:10,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5132, 'grad_norm': 69.54112243652344, 'learning_rate': 3.402777777777778e-05, 'epoch': 9.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 470/1440 [07:53<13:53,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4642, 'grad_norm': 21.091222763061523, 'learning_rate': 3.368055555555556e-05, 'epoch': 9.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 480/1440 [08:01<12:18,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5047, 'grad_norm': 33.205780029296875, 'learning_rate': 3.3333333333333335e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|███▎      | 480/1440 [08:08<12:18,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6254773139953613, 'eval_runtime': 6.7931, 'eval_samples_per_second': 17.665, 'eval_steps_per_second': 2.208, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 490/1440 [08:17<14:48,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5974, 'grad_norm': 18.97667694091797, 'learning_rate': 3.2986111111111115e-05, 'epoch': 10.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 500/1440 [08:25<13:25,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0808, 'grad_norm': 72.40280151367188, 'learning_rate': 3.263888888888889e-05, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 510/1440 [08:34<13:16,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2033, 'grad_norm': 87.89864349365234, 'learning_rate': 3.229166666666667e-05, 'epoch': 10.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 520/1440 [08:42<13:03,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3317, 'grad_norm': 45.07884979248047, 'learning_rate': 3.194444444444444e-05, 'epoch': 10.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 37%|███▋      | 528/1440 [08:55<11:31,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2196980714797974, 'eval_runtime': 6.8361, 'eval_samples_per_second': 17.554, 'eval_steps_per_second': 2.194, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 530/1440 [08:57<34:59,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.332, 'grad_norm': 63.19744110107422, 'learning_rate': 3.159722222222222e-05, 'epoch': 11.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 540/1440 [09:06<13:24,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2301, 'grad_norm': 31.810550689697266, 'learning_rate': 3.125e-05, 'epoch': 11.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 550/1440 [09:15<12:47,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4174, 'grad_norm': 53.000789642333984, 'learning_rate': 3.090277777777778e-05, 'epoch': 11.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 560/1440 [09:23<12:35,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2726, 'grad_norm': 260.8949890136719, 'learning_rate': 3.055555555555556e-05, 'epoch': 11.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 570/1440 [09:32<12:23,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1629, 'grad_norm': 98.78717041015625, 'learning_rate': 3.0208333333333334e-05, 'epoch': 11.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|████      | 576/1440 [09:43<11:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.193535327911377, 'eval_runtime': 6.8142, 'eval_samples_per_second': 17.61, 'eval_steps_per_second': 2.201, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 580/1440 [09:47<22:27,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0024, 'grad_norm': 84.19158172607422, 'learning_rate': 2.9861111111111113e-05, 'epoch': 12.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 590/1440 [09:56<12:31,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0567, 'grad_norm': 22.649805068969727, 'learning_rate': 2.951388888888889e-05, 'epoch': 12.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 600/1440 [10:04<12:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0282, 'grad_norm': 30.271682739257812, 'learning_rate': 2.916666666666667e-05, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 610/1440 [10:13<11:47,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2199, 'grad_norm': 26.20777130126953, 'learning_rate': 2.8819444444444443e-05, 'epoch': 12.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 620/1440 [10:21<11:36,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1661, 'grad_norm': 34.99270248413086, 'learning_rate': 2.8472222222222223e-05, 'epoch': 12.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 43%|████▎     | 624/1440 [10:31<10:28,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1165754795074463, 'eval_runtime': 6.8527, 'eval_samples_per_second': 17.511, 'eval_steps_per_second': 2.189, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 630/1440 [10:37<16:19,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1061, 'grad_norm': 248.42794799804688, 'learning_rate': 2.8125000000000003e-05, 'epoch': 13.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 640/1440 [10:45<11:34,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9777, 'grad_norm': 35.78262710571289, 'learning_rate': 2.777777777777778e-05, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 650/1440 [10:54<11:15,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1193, 'grad_norm': 243.92123413085938, 'learning_rate': 2.743055555555556e-05, 'epoch': 13.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 660/1440 [11:02<11:12,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0447, 'grad_norm': 42.20049285888672, 'learning_rate': 2.7083333333333332e-05, 'epoch': 13.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 670/1440 [11:11<10:53,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1196, 'grad_norm': 81.3025131225586, 'learning_rate': 2.6736111111111112e-05, 'epoch': 13.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 47%|████▋     | 672/1440 [11:19<09:49,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2014189958572388, 'eval_runtime': 6.7984, 'eval_samples_per_second': 17.651, 'eval_steps_per_second': 2.206, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 680/1440 [11:26<13:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1078, 'grad_norm': 63.138187408447266, 'learning_rate': 2.6388888888888892e-05, 'epoch': 14.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 690/1440 [11:35<10:46,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0793, 'grad_norm': 74.3315200805664, 'learning_rate': 2.604166666666667e-05, 'epoch': 14.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 700/1440 [11:43<10:38,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0994, 'grad_norm': 64.53267669677734, 'learning_rate': 2.5694444444444445e-05, 'epoch': 14.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 710/1440 [11:52<10:16,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.037, 'grad_norm': 34.40169906616211, 'learning_rate': 2.534722222222222e-05, 'epoch': 14.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 720/1440 [12:00<09:11,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1127, 'grad_norm': 32.199623107910156, 'learning_rate': 2.5e-05, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|█████     | 720/1440 [12:07<09:11,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2590124607086182, 'eval_runtime': 6.8615, 'eval_samples_per_second': 17.489, 'eval_steps_per_second': 2.186, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 730/1440 [12:16<11:05,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1339, 'grad_norm': 39.83015823364258, 'learning_rate': 2.465277777777778e-05, 'epoch': 15.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 740/1440 [12:24<09:58,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1469, 'grad_norm': 85.28556060791016, 'learning_rate': 2.4305555555555558e-05, 'epoch': 15.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 750/1440 [12:33<09:45,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0126, 'grad_norm': 50.259647369384766, 'learning_rate': 2.3958333333333334e-05, 'epoch': 15.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 760/1440 [12:41<09:37,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9847, 'grad_norm': 174.34963989257812, 'learning_rate': 2.361111111111111e-05, 'epoch': 15.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 53%|█████▎    | 768/1440 [12:55<08:38,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2369039058685303, 'eval_runtime': 6.7803, 'eval_samples_per_second': 17.698, 'eval_steps_per_second': 2.212, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 770/1440 [12:57<25:45,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0822, 'grad_norm': 48.69078063964844, 'learning_rate': 2.326388888888889e-05, 'epoch': 16.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 780/1440 [13:05<09:51,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9286, 'grad_norm': 33.14524459838867, 'learning_rate': 2.2916666666666667e-05, 'epoch': 16.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 790/1440 [13:14<09:14,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0827, 'grad_norm': 68.62828063964844, 'learning_rate': 2.2569444444444447e-05, 'epoch': 16.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 800/1440 [13:22<09:03,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9791, 'grad_norm': 373.691162109375, 'learning_rate': 2.2222222222222223e-05, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 810/1440 [13:31<08:56,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0065, 'grad_norm': 30.431598663330078, 'learning_rate': 2.1875e-05, 'epoch': 16.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 57%|█████▋    | 816/1440 [13:42<07:58,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1188652515411377, 'eval_runtime': 6.817, 'eval_samples_per_second': 17.603, 'eval_steps_per_second': 2.2, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 820/1440 [13:46<16:16,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0513, 'grad_norm': 102.23709869384766, 'learning_rate': 2.152777777777778e-05, 'epoch': 17.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 830/1440 [13:55<08:49,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1476, 'grad_norm': 32.929290771484375, 'learning_rate': 2.1180555555555556e-05, 'epoch': 17.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 840/1440 [14:03<08:28,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1659, 'grad_norm': 50.95139694213867, 'learning_rate': 2.0833333333333336e-05, 'epoch': 17.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 850/1440 [14:12<08:26,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9969, 'grad_norm': 59.39663314819336, 'learning_rate': 2.0486111111111113e-05, 'epoch': 17.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 860/1440 [14:20<08:13,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9721, 'grad_norm': 24.230016708374023, 'learning_rate': 2.013888888888889e-05, 'epoch': 17.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 60%|██████    | 864/1440 [14:30<07:20,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1468535661697388, 'eval_runtime': 6.8286, 'eval_samples_per_second': 17.573, 'eval_steps_per_second': 2.197, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 870/1440 [14:36<11:24,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0211, 'grad_norm': 34.5909423828125, 'learning_rate': 1.9791666666666665e-05, 'epoch': 18.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 880/1440 [14:44<08:01,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9473, 'grad_norm': 16.08586883544922, 'learning_rate': 1.9444444444444445e-05, 'epoch': 18.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 890/1440 [14:53<07:50,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.021, 'grad_norm': 33.62776184082031, 'learning_rate': 1.9097222222222222e-05, 'epoch': 18.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 900/1440 [15:01<07:41,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0439, 'grad_norm': 36.35734939575195, 'learning_rate': 1.8750000000000002e-05, 'epoch': 18.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 910/1440 [15:10<07:38,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9284, 'grad_norm': 38.454376220703125, 'learning_rate': 1.8402777777777778e-05, 'epoch': 18.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 63%|██████▎   | 912/1440 [15:18<06:48,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0640286207199097, 'eval_runtime': 6.8306, 'eval_samples_per_second': 17.568, 'eval_steps_per_second': 2.196, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 920/1440 [15:25<08:50,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9432, 'grad_norm': 601.6921997070312, 'learning_rate': 1.8055555555555555e-05, 'epoch': 19.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 930/1440 [15:34<07:16,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.101, 'grad_norm': 51.38807678222656, 'learning_rate': 1.7708333333333335e-05, 'epoch': 19.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 940/1440 [15:43<07:06,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0004, 'grad_norm': 69.0838623046875, 'learning_rate': 1.736111111111111e-05, 'epoch': 19.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 950/1440 [15:51<07:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8207, 'grad_norm': 22.735429763793945, 'learning_rate': 1.701388888888889e-05, 'epoch': 19.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 960/1440 [15:59<06:10,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9432, 'grad_norm': 152.88897705078125, 'learning_rate': 1.6666666666666667e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 67%|██████▋   | 960/1440 [16:06<06:10,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9668861031532288, 'eval_runtime': 6.8256, 'eval_samples_per_second': 17.581, 'eval_steps_per_second': 2.198, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 970/1440 [16:15<07:15,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9848, 'grad_norm': 20.387178421020508, 'learning_rate': 1.6319444444444444e-05, 'epoch': 20.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 980/1440 [16:24<06:53,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7271, 'grad_norm': 40.85871124267578, 'learning_rate': 1.597222222222222e-05, 'epoch': 20.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 990/1440 [16:33<06:49,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1053, 'grad_norm': 68.16870880126953, 'learning_rate': 1.5625e-05, 'epoch': 20.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 1000/1440 [16:41<06:19,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.892, 'grad_norm': 42.1186637878418, 'learning_rate': 1.527777777777778e-05, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 1008/1440 [16:55<05:34,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9494062066078186, 'eval_runtime': 6.9243, 'eval_samples_per_second': 17.33, 'eval_steps_per_second': 2.166, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1010/1440 [16:57<16:45,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8782, 'grad_norm': 28.8940372467041, 'learning_rate': 1.4930555555555557e-05, 'epoch': 21.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1020/1440 [17:06<06:22,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9544, 'grad_norm': 23.654041290283203, 'learning_rate': 1.4583333333333335e-05, 'epoch': 21.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1030/1440 [17:14<05:53,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8364, 'grad_norm': 49.06047058105469, 'learning_rate': 1.4236111111111111e-05, 'epoch': 21.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1040/1440 [17:23<05:47,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8145, 'grad_norm': 35.062347412109375, 'learning_rate': 1.388888888888889e-05, 'epoch': 21.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1050/1440 [17:31<05:34,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0527, 'grad_norm': 28.6856689453125, 'learning_rate': 1.3541666666666666e-05, 'epoch': 21.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 73%|███████▎  | 1056/1440 [17:43<04:59,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0354772806167603, 'eval_runtime': 6.8465, 'eval_samples_per_second': 17.527, 'eval_steps_per_second': 2.191, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 1060/1440 [17:47<10:04,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8875, 'grad_norm': 37.292213439941406, 'learning_rate': 1.3194444444444446e-05, 'epoch': 22.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 1070/1440 [17:56<05:28,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9531, 'grad_norm': 74.33650970458984, 'learning_rate': 1.2847222222222222e-05, 'epoch': 22.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1080/1440 [18:04<05:13,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8324, 'grad_norm': 30.681415557861328, 'learning_rate': 1.25e-05, 'epoch': 22.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1090/1440 [18:13<04:59,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9457, 'grad_norm': 53.51750183105469, 'learning_rate': 1.2152777777777779e-05, 'epoch': 22.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 1100/1440 [18:22<04:54,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8776, 'grad_norm': 196.58651733398438, 'learning_rate': 1.1805555555555555e-05, 'epoch': 22.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 77%|███████▋  | 1104/1440 [18:32<04:23,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0998197793960571, 'eval_runtime': 6.8324, 'eval_samples_per_second': 17.563, 'eval_steps_per_second': 2.195, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1110/1440 [18:37<06:46,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9974, 'grad_norm': 48.237937927246094, 'learning_rate': 1.1458333333333333e-05, 'epoch': 23.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1120/1440 [18:46<04:43,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8706, 'grad_norm': 283.6609191894531, 'learning_rate': 1.1111111111111112e-05, 'epoch': 23.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1130/1440 [18:55<04:26,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8202, 'grad_norm': 21.0698184967041, 'learning_rate': 1.076388888888889e-05, 'epoch': 23.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 1140/1440 [19:03<04:15,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8801, 'grad_norm': 29.355043411254883, 'learning_rate': 1.0416666666666668e-05, 'epoch': 23.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1150/1440 [19:12<04:08,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0621, 'grad_norm': 47.313995361328125, 'learning_rate': 1.0069444444444445e-05, 'epoch': 23.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 1152/1440 [19:20<03:42,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1395621299743652, 'eval_runtime': 6.9312, 'eval_samples_per_second': 17.313, 'eval_steps_per_second': 2.164, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1160/1440 [19:28<04:54,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8841, 'grad_norm': 30.897945404052734, 'learning_rate': 9.722222222222223e-06, 'epoch': 24.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 1170/1440 [19:36<03:58,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8426, 'grad_norm': 66.06346893310547, 'learning_rate': 9.375000000000001e-06, 'epoch': 24.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1180/1440 [19:45<03:46,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8604, 'grad_norm': 34.7971305847168, 'learning_rate': 9.027777777777777e-06, 'epoch': 24.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1190/1440 [19:54<03:34,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.817, 'grad_norm': 37.72384262084961, 'learning_rate': 8.680555555555556e-06, 'epoch': 24.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1200/1440 [20:02<03:06,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9233, 'grad_norm': 60.97060012817383, 'learning_rate': 8.333333333333334e-06, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 83%|████████▎ | 1200/1440 [20:09<03:06,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1314842700958252, 'eval_runtime': 6.9272, 'eval_samples_per_second': 17.323, 'eval_steps_per_second': 2.165, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1210/1440 [20:18<03:42,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7472, 'grad_norm': 85.20040893554688, 'learning_rate': 7.98611111111111e-06, 'epoch': 25.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 1220/1440 [20:27<03:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9319, 'grad_norm': 25.693416595458984, 'learning_rate': 7.63888888888889e-06, 'epoch': 25.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1230/1440 [20:35<03:01,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1549, 'grad_norm': 26.555145263671875, 'learning_rate': 7.2916666666666674e-06, 'epoch': 25.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1240/1440 [20:44<02:55,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7441, 'grad_norm': 67.04280853271484, 'learning_rate': 6.944444444444445e-06, 'epoch': 25.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 87%|████████▋ | 1248/1440 [20:58<02:30,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0757590532302856, 'eval_runtime': 6.8558, 'eval_samples_per_second': 17.503, 'eval_steps_per_second': 2.188, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1250/1440 [21:00<07:24,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8361, 'grad_norm': 30.013944625854492, 'learning_rate': 6.597222222222223e-06, 'epoch': 26.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1260/1440 [21:08<02:44,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.792, 'grad_norm': 34.08377456665039, 'learning_rate': 6.25e-06, 'epoch': 26.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1270/1440 [21:17<02:28,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8477, 'grad_norm': 25.06064224243164, 'learning_rate': 5.902777777777778e-06, 'epoch': 26.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 1280/1440 [21:26<02:17,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8852, 'grad_norm': 21.767162322998047, 'learning_rate': 5.555555555555556e-06, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 1290/1440 [21:35<02:10,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8867, 'grad_norm': 32.16578674316406, 'learning_rate': 5.208333333333334e-06, 'epoch': 26.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 1296/1440 [21:46<01:52,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.049934983253479, 'eval_runtime': 6.8751, 'eval_samples_per_second': 17.454, 'eval_steps_per_second': 2.182, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1300/1440 [21:50<03:43,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7943, 'grad_norm': 55.680335998535156, 'learning_rate': 4.861111111111111e-06, 'epoch': 27.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1310/1440 [21:59<01:56,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8688, 'grad_norm': 60.92213821411133, 'learning_rate': 4.513888888888889e-06, 'epoch': 27.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1320/1440 [22:08<01:43,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7761, 'grad_norm': 38.50467300415039, 'learning_rate': 4.166666666666667e-06, 'epoch': 27.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1330/1440 [22:16<01:34,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9411, 'grad_norm': 26.06531524658203, 'learning_rate': 3.819444444444445e-06, 'epoch': 27.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1340/1440 [22:25<01:26,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8215, 'grad_norm': 18.80521583557129, 'learning_rate': 3.4722222222222224e-06, 'epoch': 27.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 93%|█████████▎| 1344/1440 [22:35<01:15,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9934400320053101, 'eval_runtime': 6.9961, 'eval_samples_per_second': 17.152, 'eval_steps_per_second': 2.144, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1350/1440 [22:41<01:50,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7751, 'grad_norm': 20.518911361694336, 'learning_rate': 3.125e-06, 'epoch': 28.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1360/1440 [22:49<01:10,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7796, 'grad_norm': 25.75287628173828, 'learning_rate': 2.777777777777778e-06, 'epoch': 28.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1370/1440 [22:58<01:01,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7905, 'grad_norm': 34.657135009765625, 'learning_rate': 2.4305555555555557e-06, 'epoch': 28.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1380/1440 [23:07<00:52,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8827, 'grad_norm': 32.303287506103516, 'learning_rate': 2.0833333333333334e-06, 'epoch': 28.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1390/1440 [23:16<00:43,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.75, 'grad_norm': 33.59576416015625, 'learning_rate': 1.7361111111111112e-06, 'epoch': 28.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 97%|█████████▋| 1392/1440 [23:24<00:37,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9946499466896057, 'eval_runtime': 6.9644, 'eval_samples_per_second': 17.23, 'eval_steps_per_second': 2.154, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1400/1440 [23:31<00:41,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8176, 'grad_norm': 48.18648147583008, 'learning_rate': 1.388888888888889e-06, 'epoch': 29.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1410/1440 [23:40<00:26,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8605, 'grad_norm': 52.541893005371094, 'learning_rate': 1.0416666666666667e-06, 'epoch': 29.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 1420/1440 [23:49<00:17,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.68, 'grad_norm': 24.892900466918945, 'learning_rate': 6.944444444444445e-07, 'epoch': 29.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1430/1440 [23:57<00:08,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8595, 'grad_norm': 30.51968765258789, 'learning_rate': 3.4722222222222224e-07, 'epoch': 29.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1440/1440 [24:06<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7878, 'grad_norm': 24.62405014038086, 'learning_rate': 0.0, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1440/1440 [24:13<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9973583817481995, 'eval_runtime': 6.8952, 'eval_samples_per_second': 17.403, 'eval_steps_per_second': 2.175, 'epoch': 30.0}\n",
      "{'train_runtime': 1453.2469, 'train_samples_per_second': 9.909, 'train_steps_per_second': 0.991, 'train_loss': 1.1685521596007877, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "\n",
    "        new_ann = {\n",
    "            \"id\": image_id,\n",
    "            \"category_id\": category[i],  # Hier wird das richtige category ID verwendet\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "\n",
    "        annotations.append(new_ann)\n",
    "    return annotations\n",
    "\n",
    "# Create annotations such that they match the expected form by the algorithm\n",
    "def transform_ann(examples, image_processor = YolosImageProcessor()):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]): \n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(image)\n",
    "        bboxes.append(objects[\"bbox\"])\n",
    "        categories.append(objects[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "    {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "    for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\") # Is applied on the whole batch\n",
    "\n",
    "def collate_fn(batch, image_processor = YolosImageProcessor()):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    #batch[\"pixel_mask\"] = encoding[\"pixel_mask\"] # For object detection we do not need this - only needed for segmentation.\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "def model_training(categories, model_name, train_data, validation_data, num_epochs=3, image_processor=YolosImageProcessor(), output_name=\"Kugellager\"):\n",
    "    # Mapping zwischen IDs und Labels\n",
    "    id2label = {category['id']: category['name'] for category in categories}\n",
    "    label2id = {category['name']: category['id'] for category in categories} \n",
    "\n",
    "    # Modell initialisieren\n",
    "    model = AutoModelForObjectDetection.from_pretrained(\n",
    "        model_name,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,  \n",
    "    )\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\") \n",
    "\n",
    "    if device == \"cpu\":\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir=f\"trained_model/{output_name}/{model_name}\", \n",
    "        remove_unused_columns=False, \n",
    "        load_best_model_at_end=False, \n",
    "        save_strategy=\"no\", \n",
    "        eval_strategy=\"epoch\", \n",
    "        per_device_train_batch_size=2, \n",
    "        push_to_hub=False,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir=f\"trained_model/{output_name}/{model_name}\", \n",
    "        remove_unused_columns=False, \n",
    "        load_best_model_at_end=False, \n",
    "        save_strategy=\"no\", \n",
    "        eval_strategy=\"epoch\", \n",
    "        per_device_train_batch_size=10, \n",
    "        push_to_hub=False,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "    # Hier wird dafür gesorgt, dass das Modell auch auf der Grafikkarte trainert werden kann, wenn eine vorhanden sein sollte. \n",
    "    model.to(device)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn, \n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=image_processor,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    evaluation_results = trainer.evaluate()\n",
    "\n",
    "    # Plotten der Losses aus dem TrainerState\n",
    "    train_logs = trainer.state.log_history\n",
    "\n",
    "    # Extrahiere Training und Evaluation Losses\n",
    "    train_losses = [log['loss'] for log in train_logs if 'loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in train_logs if 'eval_loss' in log]\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "    \n",
    "    if len(eval_losses) > 0:\n",
    "        # Falls unterschiedliche Längen, interpoliere oder schneide\n",
    "        if len(eval_losses) != len(train_losses):\n",
    "            # Interpoliere eval_losses auf die Länge von train_losses\n",
    "            x_train = np.linspace(0, len(train_losses)-1, len(train_losses))\n",
    "            x_eval = np.linspace(0, len(train_losses)-1, len(eval_losses))\n",
    "            eval_losses_interpolated = np.interp(x_train, x_eval, eval_losses)\n",
    "            \n",
    "            plt.plot(eval_losses_interpolated, label='Evaluation Loss', color='red')\n",
    "        else:\n",
    "            plt.plot(eval_losses, label='Evaluation Loss', color='red')\n",
    "\n",
    "    plt.title(f'Loss Progression for {model_name}')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"trained_model/{output_name}/{model_name}/loss_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    save_path = f\"trained_model/{output_name}/{model_name}/final_model\"\n",
    "    model.save_pretrained(save_path)\n",
    "\n",
    "    return evaluation_results, train_losses, eval_losses\n",
    "    \n",
    "\n",
    "# Transform data such that it can be feed to the model\n",
    "train_data_kugellager = dataset_kugellager_train.with_transform(transform_ann)\n",
    "test_data_kugellager = dataset_kugellager_test.with_transform(transform_ann)\n",
    "\n",
    "train_data_oberfläche = dataset_oberfläche_train.with_transform(transform_ann)\n",
    "test_data_oberfläche = dataset_oberfläche_test.with_transform(transform_ann)\n",
    "\n",
    "\n",
    "train_data_kugellager_halb = dataset_kugellager_train_halb.with_transform(transform_ann)\n",
    "test_data_kugellager_halb = dataset_kugellager_test_halb.with_transform(transform_ann)\n",
    "\n",
    "train_data_oberfläche_halb = dataset_oberfläche_train_halb.with_transform(transform_ann)\n",
    "test_data_oberfläche_halb = dataset_oberfläche_test_halb.with_transform(transform_ann)\n",
    "\n",
    "best_models_kugellager_dict = {}\n",
    "best_models_oberfläche_dict = {}\n",
    "\n",
    "best_models_kugellager_dict_halb = {}\n",
    "best_models_oberfläche_dict_halb = {}\n",
    "\n",
    "# \"ultralytics/yolov11-m\", \n",
    "# \"jparedesDS/welding-defects-detection\",\n",
    "# \"facebook/detr-resnet-50\", \n",
    "# \"hustvl/yolos-small\",\n",
    "# \"hustvl/yolos-tiny\"\n",
    "# \"jparedesDS/welding-defects-detection\",\n",
    "# \"hustvl/yolos-small\",\n",
    "# \"hustvl/yolos-tiny\",\n",
    "model_training_list = [ \"facebook/detr-resnet-50\", ]\n",
    "\n",
    "for model_name in model_training_list:\n",
    "    evaluation_results, train_losses, eval_losses = model_training(\n",
    "        categories=categories_kugellager, \n",
    "        model_name=model_name, \n",
    "        train_data=train_data_kugellager, \n",
    "        validation_data=test_data_kugellager, \n",
    "        num_epochs=30,\n",
    "        output_name=\"Kugellager\"\n",
    "    )\n",
    "    best_models_kugellager_dict[model_name] = {\n",
    "        'evaluation_results': evaluation_results,\n",
    "        'train_losses': train_losses,\n",
    "        'eval_losses': eval_losses\n",
    "    }\n",
    "\n",
    "\n",
    "# for model_name in model_training_list:\n",
    "#     evaluation_results, train_losses, eval_losses = model_training(\n",
    "#         categories=categories_oberfläche, \n",
    "#         model_name=model_name, \n",
    "#         train_data=train_data_oberfläche, \n",
    "#         validation_data=test_data_oberfläche, \n",
    "#         num_epochs=30,\n",
    "#         output_name=\"Oberfläche\"\n",
    "#     )\n",
    "#     best_models_oberfläche_dict[model_name] = {\n",
    "#         'evaluation_results': evaluation_results,\n",
    "#         'train_losses': train_losses,\n",
    "#         'eval_losses': eval_losses\n",
    "#     }\n",
    "\n",
    "\n",
    "# for model_name in model_training_list:\n",
    "#     evaluation_results, train_losses, eval_losses = model_training(\n",
    "#         categories=categories_kugellager, \n",
    "#         model_name=model_name, \n",
    "#         train_data=train_data_kugellager_halb, \n",
    "#         validation_data=test_data_kugellager_halb, \n",
    "#         num_epochs=30,\n",
    "#         output_name=\"Kugellager_halb\"\n",
    "#     )\n",
    "#     best_models_kugellager_dict_halb[model_name] = {\n",
    "#         'evaluation_results': evaluation_results,\n",
    "#         'train_losses': train_losses,\n",
    "#         'eval_losses': eval_losses\n",
    "#     }\n",
    "\n",
    "\n",
    "# for model_name in model_training_list:\n",
    "#     evaluation_results, train_losses, eval_losses = model_training(\n",
    "#         categories=categories_oberfläche, \n",
    "#         model_name=model_name, \n",
    "#         train_data=train_data_oberfläche_halb, \n",
    "#         validation_data=test_data_oberfläche_halb, \n",
    "#         num_epochs=30,\n",
    "#         output_name=\"Oberfläche_halb\"\n",
    "#     )\n",
    "#     best_models_oberfläche_dict_halb[model_name] = {\n",
    "#         'evaluation_results': evaluation_results,\n",
    "#         'train_losses': train_losses,\n",
    "#         'eval_losses': eval_losses\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(img, results, output_dir=None):\n",
    "    if isinstance(img, str):\n",
    "        image = Image.open(img)\n",
    "    else:\n",
    "        image = img\n",
    "    image = image.copy()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    for obj in results:\n",
    "        box = obj[\"box\"]\n",
    "        x = int(box[\"xmin\"])\n",
    "        y = int(box[\"ymin\"])\n",
    "        x2 = int(box[\"xmax\"])\n",
    "        y2 = int(box[\"ymax\"])\n",
    "        \n",
    "        draw.rectangle([(x, y), (x2, y2)], outline=\"orange\", width=2)\n",
    "        \n",
    "        label_text = f\"{obj['label']} - {np.round(obj['score'], 3)}\"\n",
    "        draw.text((x, y - 20), label_text, fill=\"orange\", stroke_width=2, stroke_fill=\"white\")\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f\"detection_result_{np.random.randint(10e6)}.jpg\")\n",
    "        image.save(output_path)\n",
    "        return output_path\n",
    "    return np.array(image) \n",
    "\n",
    "def load_model_for_inference(model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"Load the trained model and move it to the specified device\"\"\"\n",
    "    model = AutoModelForObjectDetection.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_predictions(model, image, image_processor=YolosImageProcessor(), confidence_threshold=0.5):\n",
    "    \"\"\"Process an image and return predictions with timing information\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image)\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    # Preprocessing timing\n",
    "    preprocess_start = time.time()\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    preprocess_time = time.time() - preprocess_start\n",
    "    \n",
    "    # Inference timing\n",
    "    inference_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    inference_time = time.time() - inference_start\n",
    "    \n",
    "    # Post-processing timing\n",
    "    postprocess_start = time.time()\n",
    "    results = []\n",
    "    \n",
    "    target_size = image_processor.post_process_object_detection(\n",
    "        outputs, \n",
    "        target_sizes=[(image.size[1], image.size[0])],\n",
    "        threshold=confidence_threshold\n",
    "    )[0]\n",
    "    \n",
    "    for score, label, box in zip(\n",
    "        target_size[\"scores\"].tolist(),\n",
    "        target_size[\"labels\"].tolist(),\n",
    "        target_size[\"boxes\"].tolist()\n",
    "    ):\n",
    "        if score >= confidence_threshold:\n",
    "            results.append({\n",
    "                \"score\": score,\n",
    "                \"label\": model.config.id2label[label],\n",
    "                \"box\": {\n",
    "                    \"xmin\": box[0],\n",
    "                    \"ymin\": box[1],\n",
    "                    \"xmax\": box[2],\n",
    "                    \"ymax\": box[3]\n",
    "                }\n",
    "            })\n",
    "    postprocess_time = time.time() - postprocess_start\n",
    "    \n",
    "    # Total time\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    timing_info = {\n",
    "        \"preprocess_time\": preprocess_time,\n",
    "        \"inference_time\": inference_time,\n",
    "        \"postprocess_time\": postprocess_time,\n",
    "        \"total_time\": total_time\n",
    "    }\n",
    "    \n",
    "    return results, timing_info\n",
    "\n",
    "def detect_and_visualize(model_path, image_path, output_dir, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Führt die nötigen Funktionen aus um ein paar test-Bilder zu lablen un gibt Informaitonen über das Timing aus.\n",
    "    \"\"\"\n",
    "    model = load_model_for_inference(model_path)\n",
    "    predictions, timing_info = get_predictions(model, image_path, confidence_threshold=confidence_threshold)\n",
    "    output_path = plot_box(image_path, predictions, output_dir)\n",
    "    \n",
    "    print(f\"Processing times:\")\n",
    "    print(f\"Preprocessing: {timing_info['preprocess_time']*1000:.1f}ms\")\n",
    "    print(f\"Inference: {timing_info['inference_time']*1000:.1f}ms\")\n",
    "    print(f\"Postprocessing: {timing_info['postprocess_time']*1000:.1f}ms\")\n",
    "    print(f\"Total time: {timing_info['total_time']*1000:.1f}ms\")\n",
    "    \n",
    "    return output_path, timing_info\n",
    "\n",
    "\n",
    "def load_random_images(image_folder, num_images=10):\n",
    "    \"\"\"\n",
    "    Lädt zufällig ausgewählte Bilder aus einem Ordner.\n",
    "    \"\"\"\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
    "    selected_files = random.sample(image_files, min(len(image_files), num_images))\n",
    "    return [os.path.join(image_folder, f) for f in selected_files]\n",
    "\n",
    "\n",
    "model_path_oberfläche_yolo = os.path.join(BASE_DIR, \"trained_model\", \"Oberfläche\", \"hustvl\", \"yolos-tiny\", \"final_model\")\n",
    "model_path_oberfläche_resnet = os.path.join(BASE_DIR, \"trained_model\", \"Oberfläche\", \"facebook\", \"detr-resnet-50\", \"final_model\")\n",
    "list_pics_oberfläche = load_random_images(images_dir_oberfläche_test)\n",
    "for pic in list_pics_oberfläche:\n",
    "    detect_and_visualize(model_path=model_path_oberfläche_yolo, image_path=pic, output_dir=\"test_labeling_pics_oberfläche_yolo\")\n",
    "    detect_and_visualize(model_path=model_path_oberfläche_resnet, image_path=pic, output_dir=\"test_labeling_pics_oberfläche_resnet\", confidence_threshold=0.4)\n",
    "\n",
    "\n",
    "model_path_kugellager_yolo = os.path.join(BASE_DIR, \"trained_model\", \"Kugellager\", \"hustlv\", \"yolos-tiny\", \"final_model\")\n",
    "model_path_kugellager_yolo = r\"C:\\Users\\anohl\\OneDrive\\Dokumente\\A_Uni_stuff\\Albstadt\\Semester 2\\Computer_vision\\Aufgaben\\trained_model\\Oberfläche\\hustvl\\yolos-tiny\\final_model\"\n",
    "model_path_kugellager_resnet = os.path.join(BASE_DIR, \"trained_model\", \"Kugellager\", \"facebook\", \"detr-resnet-50\", \"final_model\")\n",
    "list_pics_kugellager = load_random_images(images_dir_kugellager_test)\n",
    "for pic in list_pics_kugellager:\n",
    "    detect_and_visualize(model_path=model_path_kugellager_yolo, image_path=pic, output_dir=\"test_labeling_pics_kugellager_yolo\")\n",
    "    detect_and_visualize(model_path=model_path_kugellager_resnet, image_path=pic, output_dir=\"test_labeling_pics_kugellager_resnet\", confidence_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Opening camera...\n",
      "Camera opened successfully!\n",
      "Starting video stream. Press 'q' to quit.\n",
      "Quitting...\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "def run_video_detection(model_path, camera_id=0, confidence_threshold=0.5):\n",
    "    \"\"\"Run real-time object detection on video stream\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model_for_inference(model_path)\n",
    "    image_processor = YolosImageProcessor()\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    print(\"Opening camera...\")\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open camera with ID {camera_id}\")\n",
    "    print(\"Camera opened successfully!\")\n",
    "    \n",
    "    # Create window\n",
    "    window_name = 'Object Detection'\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    # Initialize FPS counter\n",
    "    fps_start_time = time.time()\n",
    "    fps_counter = 0\n",
    "    fps = 0\n",
    "    \n",
    "    print(\"Starting video stream. Press 'q' to quit.\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(rgb_frame)\n",
    "            \n",
    "            # Get predictions with timing\n",
    "            predictions, timing_info = get_predictions(model, pil_image, \n",
    "                                                    image_processor=image_processor,\n",
    "                                                    confidence_threshold=confidence_threshold)\n",
    "            \n",
    "            # Draw boxes\n",
    "            result_image = plot_box(pil_image, predictions)\n",
    "            \n",
    "            # Convert back to BGR for OpenCV\n",
    "            result_frame = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Calculate and display FPS\n",
    "            fps_counter += 1\n",
    "            if time.time() - fps_start_time > 1:\n",
    "                fps = fps_counter\n",
    "                fps_counter = 0\n",
    "                fps_start_time = time.time()\n",
    "            \n",
    "            # Add timing information to frame\n",
    "            cv2.putText(result_frame, f\"FPS: {fps}\", (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            cv2.putText(result_frame, f\"Inference: {timing_info['inference_time']*1000:.1f}ms\", \n",
    "                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the frame\n",
    "            cv2.imshow(window_name, result_frame)\n",
    "            \n",
    "            # Break on 'q' press\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                print(\"Quitting...\")\n",
    "                break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        print(\"Cleaning up...\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage for single image with timing\n",
    "def detect_and_visualize(model_path, image_path, output_dir, confidence_threshold=0.5):\n",
    "    model = load_model_for_inference(model_path)\n",
    "    predictions, timing_info = get_predictions(model, image_path, confidence_threshold=confidence_threshold)\n",
    "    output_path = plot_box(image_path, predictions, output_dir)\n",
    "    \n",
    "    print(f\"Processing times:\")\n",
    "    print(f\"Preprocessing: {timing_info['preprocess_time']*1000:.1f}ms\")\n",
    "    print(f\"Inference: {timing_info['inference_time']*1000:.1f}ms\")\n",
    "    print(f\"Postprocessing: {timing_info['postprocess_time']*1000:.1f}ms\")\n",
    "    print(f\"Total time: {timing_info['total_time']*1000:.1f}ms\")\n",
    "    \n",
    "    return output_path, timing_info\n",
    "\n",
    "\n",
    "\n",
    "run_video_detection(model_path_kugellager_yolo, camera_id=0, confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: facebook/detr-resnet-50, performte mit folgenden Scores:\n",
      "{'evaluation_results': {'eval_loss': 0.9217938780784607, 'eval_runtime': 6.6457, 'eval_samples_per_second': 18.057, 'eval_steps_per_second': 2.257, 'epoch': 30.0}, 'train_losses': [2.1437, 2.1104, 1.8866, 1.4919, 1.3816, 1.4724, 1.346, 1.3652, 1.29, 1.3952, 1.4707, 1.7389, 1.882, 1.7675, 2.09, 1.5142, 1.3907, 1.4731, 1.7424, 1.5529, 1.4501, 1.6311, 1.3621, 1.3207, 1.325, 1.1566, 1.2321, 1.1528, 1.1561, 1.4008, 1.3711, 1.2979, 1.2548, 1.1528, 1.4543, 1.2188, 1.1515, 1.2485, 1.2591, 1.1641, 1.3861, 1.0755, 1.163, 1.194, 1.115, 0.9685, 1.1241, 1.1063, 1.225, 1.0863, 1.116, 1.1841, 1.0292, 0.939, 1.1251, 1.1066, 1.1021, 0.9601, 0.9873, 1.1667, 1.2175, 1.2583, 1.1623, 1.0874, 1.1932, 1.1639, 1.0901, 1.1069, 1.0272, 1.0402, 1.0037, 1.0132, 0.9903, 1.0413, 0.912, 0.8118, 0.9497, 0.8802, 0.9274, 0.969, 0.9352, 0.9586, 0.9883, 0.9851, 0.9245, 0.8756, 0.971, 0.8926, 1.1114, 0.9714, 0.9641, 0.873, 0.9988, 0.9649, 0.9028, 0.8621, 1.0643, 0.8306, 1.0284, 0.8861, 0.8929, 1.0067, 0.8658, 0.8808, 1.1036, 0.8653, 0.9697, 0.8627, 0.8737, 0.8419, 0.9257, 0.8627, 0.8534, 0.7836, 0.9571, 0.8353, 0.8403, 0.8591, 0.7485, 0.8823, 0.7448, 0.9004, 0.963, 0.7984, 0.8339, 0.8263, 0.8262, 0.8119, 0.8347, 0.8683, 0.8329, 0.7714, 0.8882, 0.8019, 0.7935, 0.7645, 0.7317, 0.8823, 0.7849, 0.7479, 0.8602, 0.6869, 0.8119, 0.7663], 'eval_losses': [1.277187705039978, 1.6899189949035645, 2.23893666267395, 1.9064829349517822, 1.4268043041229248, 1.1834368705749512, 1.3977713584899902, 1.1986801624298096, 1.1508408784866333, 1.056110143661499, 1.0013294219970703, 1.240343451499939, 1.1826287508010864, 1.0626442432403564, 0.977898895740509, 0.9773236513137817, 0.9900186061859131, 1.0640472173690796, 0.9862998723983765, 1.0487818717956543, 1.0615917444229126, 1.0207312107086182, 0.9562033414840698, 0.9539878368377686, 0.9750841856002808, 0.9642249345779419, 0.9388570189476013, 0.9604141116142273, 0.9208709001541138, 0.9217938780784607, 0.9217938780784607]}\n",
      "model: facebook/detr-resnet-50, performte mit folgenden Scores:\n",
      "{'evaluation_results': {'eval_loss': 2.703200578689575, 'eval_runtime': 6.6684, 'eval_samples_per_second': 17.995, 'eval_steps_per_second': 2.249, 'epoch': 30.0}, 'train_losses': [4.6746, 4.3132, 3.4278, 2.8593, 2.5792, 2.596, 2.5429, 2.4819, 2.3475, 2.2264, 2.1984, 2.5366, 2.4032, 2.3265, 2.3399, 2.1198, 2.1625, 2.1945, 2.2759, 2.2185, 2.1443, 2.1945, 2.019, 1.982, 1.9171, 2.1557, 2.1643, 2.0555, 2.3052, 2.4308, 2.1464, 2.0007, 1.9337, 1.9314, 2.133, 2.0866, 1.9184, 1.8221, 1.9616, 1.9265, 1.8052, 1.7788, 1.8539, 1.7654, 1.8257, 1.822, 1.7229, 1.6468, 1.805, 1.8388, 1.6346, 1.6867, 1.7521, 1.7245, 1.6616, 1.7081, 1.6469, 1.7025, 1.646, 1.6793, 1.6547, 1.7177, 1.6524, 1.58, 1.6472, 1.6014, 1.5522, 1.6189, 1.7078, 1.6665, 1.5665, 1.4957, 1.5586, 1.5786, 1.5072, 1.5874, 1.5017, 1.4495, 1.4581, 1.5494, 1.4733, 1.5226, 1.3802, 1.5727, 1.5022, 1.4507, 1.4588, 1.4022, 1.5314, 1.4423], 'eval_losses': [3.2215707302093506, 3.2545976638793945, 2.972907781600952, 4.060675621032715, 2.834340810775757, 3.2811172008514404, 2.7086215019226074, 3.136824131011963, 3.0918350219726562, 2.8745007514953613, 2.796023368835449, 2.8119845390319824, 2.8952698707580566, 2.8209657669067383, 2.8236775398254395, 2.928908586502075, 2.9061057567596436, 2.7652242183685303, 2.875002861022949, 2.9612045288085938, 2.971139430999756, 2.862086057662964, 2.7859065532684326, 2.665994167327881, 2.66984486579895, 2.6518380641937256, 2.6940081119537354, 2.6992881298065186, 2.6927525997161865, 2.703200578689575, 2.703200578689575]}\n"
     ]
    }
   ],
   "source": [
    "for model_name, score in best_models_kugellager_dict.items():\n",
    "    print(f\"Model über alle Daten: {model_name}, performte mit folgenden Scores:\\n{score}\")\n",
    "\n",
    "for model_name, score in best_models_oberfläche_dict.items():\n",
    "    print(f\"Model über alle Daten: {model_name}, performte mit folgenden Scores:\\n{score}\")\n",
    "\n",
    "for model_name, score in best_models_kugellager_dict_halb.items():\n",
    "    print(f\"Model über hälfte der Daten: {model_name}, performte mit folgenden Scores:\\n{score}\")\n",
    "\n",
    "for model_name, score in best_models_oberfläche_dict_halb.items():\n",
    "    print(f\"Model über hälfte der Daten: {model_name}, performte mit folgenden Scores:\\n{score}\")\n",
    "\n",
    "\n",
    "with open(\"model_performance_results.txt\", \"w\") as f:\n",
    "    # Ergebnisse für best_models_kugellager_dict\n",
    "    for model_name, score in best_models_kugellager_dict.items():\n",
    "        f.write(f\"Model über alle Daten: {model_name}, performte mit folgenden Scores:\\n{score}\\n\\n\")\n",
    "\n",
    "    # Ergebnisse für best_models_oberfläche_dict\n",
    "    for model_name, score in best_models_oberfläche_dict.items():\n",
    "        f.write(f\"Model über alle Daten: {model_name}, performte mit folgenden Scores:\\n{score}\\n\\n\")\n",
    "\n",
    "    # Ergebnisse für best_models_kugellager_dict_halb\n",
    "    for model_name, score in best_models_kugellager_dict_halb.items():\n",
    "        f.write(f\"Model über hälfte der Daten: {model_name}, performte mit folgenden Scores:\\n{score}\\n\\n\")\n",
    "\n",
    "    # Ergebnisse für best_models_oberfläche_dict_halb\n",
    "    for model_name, score in best_models_oberfläche_dict_halb.items():\n",
    "        f.write(f\"Model über hälfte der Daten: {model_name}, performte mit folgenden Scores:\\n{score}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6896"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize() \n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoImageProcessor, pipeline\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# random_images = random.sample(dataset_oberfläche_test[\"image\"], 10)\n",
    "# for test_image in random_images:\n",
    "#     test = r\"C:\\Users\\anohl\\OneDrive\\Dokumente\\A_Uni_stuff\\Albstadt\\Semester 2\\Computer_vision\\Aufgaben\\trained_model\\hustvl\\yolos-tiny\\final_model\"\n",
    "#     out = pipeline(\"object-detection\", model=test, image_processor=YolosImageProcessor())(test_image, threshold=0)\n",
    "\n",
    "#     # Normalize the prediction thresholds\n",
    "#     outs = [o[\"score\"] for o in out]\n",
    "#     filtered_out = []\n",
    "#     threshold = 0.9\n",
    "#     for o in out:\n",
    "#         o[\"score\"] = 1 / max(outs) * o[\"score\"]\n",
    "#         if o[\"score\"] >= threshold:\n",
    "#             filtered_out.append(o)\n",
    "#             print(filtered_out)\n",
    "\n",
    "#     def plot_box(img, results):\n",
    "#         try:\n",
    "#             image = Image.open(img)\n",
    "#         except:\n",
    "#             image = img\n",
    "#         draw = ImageDraw.Draw(image)\n",
    "#         for i, obj in enumerate(results):\n",
    "#             box = [round(obj[\"box\"][value], 2) for value in obj[\"box\"].keys()]\n",
    "#             x, y, x2, y2 = tuple(box)\n",
    "#             draw.rectangle((x, y, x2, y2), outline=\"orange\", width=2)\n",
    "#             draw.text((x, y), f\"{obj['label']} - {np.round(obj['score'], 3)}\", fill=\"orange\", stroke_width=2, stroke_fill=\"white\")\n",
    "#         image.save(f\"./{np.random.randint(10e6)}.jpg\")\n",
    "\n",
    "#     # Visualize Results\n",
    "#     plot_box(test_image, filtered_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Verwendung von yolv8m um eine Blance zwischen performance und Genauigkeit zu haben \n",
    "# model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "# # Train the model\n",
    "# path_to_yolo_yaml = os.path.join(BASE_DIR, \"yaml_files\", \"yolo_dataset.yaml\")\n",
    "# train_results = model.train(\n",
    "#     data=path_to_yolo_yaml,\n",
    "#     epochs=400,\n",
    "#     imgsz=150, # Weil das die tatsächliche Größe unserer Bilder darstellt\n",
    "#     device=device,\n",
    "#     batch=16,\n",
    "#     mosaic=1.0,\n",
    "# )\n",
    "\n",
    "# # Evaluate model performance on the validation set\n",
    "# metrics = model.val()\n",
    "# print(metrics)\n",
    "\n",
    "# # Export the model to the same directory as the script\n",
    "# export_path = os.path.join(BASE_DIR, \"Models\", \"yolo_kugellager_modell.pt\")\n",
    "# model_path = model.export()\n",
    "# shutil.move(src=model_path, dst=export_path)\n",
    "# print(f\"Model exported to: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = model.val()\n",
    "# print(f\"Precision: {metrics['precision']}\")\n",
    "# print(f\"Recall: {metrics['recall']}\")\n",
    "# print(f\"mAP@0.5: {metrics['map50']}\")\n",
    "# print(f\"mAP@0.5:0.95: {metrics['map']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_class_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
